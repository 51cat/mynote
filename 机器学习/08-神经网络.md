# 神经网络

线性模型的一般公式：

![image-20240624170817292](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240624170817292.png)

其中*ŷ*表示对*y*的估计值，*x*[0]到*x*[*p*]是样本特征值，*w*表示每个特征值的权重，*y*-hat可以看成是所有特征值的加权求和，可以通过下面的图进行表征：

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240624170856597.png" alt="image-20240624170856597" style="zoom:80%;" />

神经网络MLP算法：上面的图是输入的特征和预测的结果用节点进行表示，系数*w*用来连接这些节点。**在MLP模型中，算法在过程里添加了隐藏层（Hidden Layers），然后在隐藏层重复进行上述加权求和计算，最后再把隐藏层所计算的结果用来生成最终结果**，模型要学习的特征系数，或者说权重，就会多很多了。大家可以看到在每一个输入的特征和隐藏单元（hidden unit）之间，都有一个系数，这一步也是为了生成这些隐藏单元。而每个隐藏单元到最终结果之间，也都有一个系数。而计算一系列的加权求和和计算单一的加权求和

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240624171018857.png" alt="image-20240624171018857" style="zoom: 50%;" />

这样一来公式就会变成下面的样子，其中的tanh是一种特征转换的算法：

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240624171103807.png" alt="image-20240624171103807" style="zoom: 67%;" />

## 特征的非线性转换

一般来说如果在隐藏层里仅仅是做简单的线性求和，理论上结果与普通的线性模型是一致的，所以在隐藏层中都会对特征进行非线性矫正，在生成隐藏层之后，我们要对结果进行非线性矫正（rectifying nonlinearity），简称为relu（rectified linear unit）或者是进行双曲正切处理（tangens hyperbolicus），简称为tanh

```python
import numpy as np
import matplotlib.pyplot as plt 
line = np.linspace(-5, 5, 200)
plt.plot(line,np.tanh(line), label = "tanh")
plt.plot(line,np.maximum(line,0), label = "relu")
plt.legend()
```



![image-20240624171924431](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240624171924431.png)

tanh函数把特征*x*的值压缩进-1到1的区间内，-1代表的是*x*中较小的数值，而1代表*x*中较大的数值。relu函数则索性把小于0的*x*值全部去掉，用0来代替。这两种非线性处理的方法，都是为了将样本特征进行简化，每一次经过隐藏层都会对参数去做一个简化，从而使神经网络可以对复杂的非线性数据集进行学习

![image-20240624172104320](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240624172104320.png)

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240624172217029.png" alt="image-20240624172217029" style="zoom: 50%;" />

权重系数*v*，用来通过隐藏层*h*来计算*y*-hat的结果。在模型中，*w*和*v*都是通过对数据的学习所得出的。而用户所要设置的参数，就是隐藏层中节点的数量。一般来讲，对于小规模数据集或者简单数据集，节点数量设置为10就已经足够了，但是对于大规模数据集或者复杂数据集来说，有两种方式可供选择：一是增加隐藏层中的节点数量，比如增加到1万个；或是添加更多的隐藏层

## 酒数据集（测试）

```python
from sklearn.neural_network import MLPClassifier
from sklearn.datasets import load_wine
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

wine = load_wine()
X = wine.data[:,:2]
y = wine.target

X_train, X_test, y_train, y_test = train_test_split(X, y , random_state=0)
mlp = MLPClassifier(solver="lbfgs")
mlp.fit(X_train, y_train)

# 查看分类效果
x_min, x_max = X_train[:,0].min()-1,  X_train[:,0].max()+1
y_min, y_max = X_train[:,1].min()-1,  X_train[:,1].max()+1
# 创建网格
xx, yy = np.meshgrid(
    np.arange(x_min, x_max, 0.02),
    np.arange(y_min, y_max, 0.02),
)
# 展开数据，预测
Z = mlp.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
# 可视化
plt.pcolormesh(xx, yy, Z, alpha=0.2)
plt.scatter(X_train[:,0], X_train[:,1], c = y_train)
```

**hidden_layer_sizes参数**

值是[100,]这意味着模型中只有一个隐藏层，而隐藏层中的节点数是100。如果我们给hidden_layer_sizes定义为[10,10]，那就意味着模型中有两个隐藏层，每层有10个节点

**关于slover参数：**

在机器学习库scikit-learn中的`MLPClassifier`类中，`solver`参数用于指定优化算法，用于训练多层感知器（MLP）模型。`solver`参数的可选值包括：

- "lbfgs"：使用L-BFGS算法，这是一种拟牛顿法，适用于小到中等规模的问题。
- "sgd"：随机梯度下降（Stochastic Gradient Descent），适用于大规模数据集。
- "adam"：一种自适应学习率的优化算法，通常在训练深度学习模型时使用。
- "nesterovs_momentum"：Nesterov加速的SGD，通常可以更快地收敛。

当你设置`solver="lbfgs"`时，你选择了L-BFGS算法作为优化器。L-BFGS是一种高效的优化算法，适用于求解大规模优化问题，特别是当数据集不是非常大时。它是一种基于梯度的优化方法，利用了二阶导数（即Hessian矩阵的近似）来加速收敛。

**关于activation激活函数**

```
activation : {'identity', 'logistic', 'tanh', 'relu'}, default='relu'
    Activation function for the hidden layer.

    - 'identity', no-op activation, useful to implement linear bottleneck,
      returns f(x) = x

    - 'logistic', the logistic sigmoid function,
      returns f(x) = 1 / (1 + exp(-x)).

    - 'tanh', the hyperbolic tan function,
      returns f(x) = tanh(x).

    - 'relu', the rectified linear unit function,
      returns f(x) = max(0, x)
```

查看一下分类的效果，一层隐藏层，10个节点，activation为'relu'，对于分类，除了增加单个隐藏层中的节点数之外，还有两种方法可以让决定边界更细腻：一个是增加隐藏层的数量；另一个是把activation参数改为tanh

![image-20240624175611458](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240624175611458.png)

## 手写数据集（实例）

