# 神经网络

线性模型的一般公式：

![image-20240624170817292](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240624170817292.png)

其中*ŷ*表示对*y*的估计值，*x*[0]到*x*[*p*]是样本特征值，*w*表示每个特征值的权重，*y*-hat可以看成是所有特征值的加权求和，可以通过下面的图进行表征：

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240624170856597.png" alt="image-20240624170856597" style="zoom:80%;" />

神经网络MLP算法：上面的图是输入的特征和预测的结果用节点进行表示，系数*w*用来连接这些节点。**在MLP模型中，算法在过程里添加了隐藏层（Hidden Layers），然后在隐藏层重复进行上述加权求和计算，最后再把隐藏层所计算的结果用来生成最终结果**，模型要学习的特征系数，或者说权重，就会多很多了。大家可以看到在每一个输入的特征和隐藏单元（hidden unit）之间，都有一个系数，这一步也是为了生成这些隐藏单元。而每个隐藏单元到最终结果之间，也都有一个系数。而计算一系列的加权求和和计算单一的加权求和

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240624171018857.png" alt="image-20240624171018857" style="zoom: 50%;" />

这样一来公式就会变成下面的样子，其中的tanh是一种特征转换的算法：

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240624171103807.png" alt="image-20240624171103807" style="zoom: 67%;" />

## 特征的非线性转换

一般来说如果在隐藏层里仅仅是做简单的线性求和，理论上结果与普通的线性模型是一致的，所以在隐藏层中都会对特征进行非线性矫正，在生成隐藏层之后，我们要对结果进行非线性矫正（rectifying nonlinearity），简称为relu（rectified linear unit）或者是进行双曲正切处理（tangens hyperbolicus），简称为tanh

```python
import numpy as np
import matplotlib.pyplot as plt 
line = np.linspace(-5, 5, 200)
plt.plot(line,np.tanh(line), label = "tanh")
plt.plot(line,np.maximum(line,0), label = "relu")
plt.legend()
```



![image-20240624171924431](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240624171924431.png)

tanh函数把特征*x*的值压缩进-1到1的区间内，-1代表的是*x*中较小的数值，而1代表*x*中较大的数值。relu函数则索性把小于0的*x*值全部去掉，用0来代替。这两种非线性处理的方法，都是为了将样本特征进行简化，每一次经过隐藏层都会对参数去做一个简化，从而使神经网络可以对复杂的非线性数据集进行学习

![image-20240624172104320](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240624172104320.png)

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240624172217029.png" alt="image-20240624172217029" style="zoom: 50%;" />

权重系数*v*，用来通过隐藏层*h*来计算*y*-hat的结果。在模型中，*w*和*v*都是通过对数据的学习所得出的。而用户所要设置的参数，就是隐藏层中节点的数量。一般来讲，对于小规模数据集或者简单数据集，节点数量设置为10就已经足够了，但是对于大规模数据集或者复杂数据集来说，有两种方式可供选择：一是增加隐藏层中的节点数量，比如增加到1万个；或是添加更多的隐藏层

## 酒数据集（测试）

```python
from sklearn.neural_network import MLPClassifier
from sklearn.datasets import load_wine
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

wine = load_wine()
X = wine.data[:,:2]
y = wine.target

X_train, X_test, y_train, y_test = train_test_split(X, y , random_state=0)
mlp = MLPClassifier(solver="lbfgs")
mlp.fit(X_train, y_train)

# 查看分类效果
x_min, x_max = X_train[:,0].min()-1,  X_train[:,0].max()+1
y_min, y_max = X_train[:,1].min()-1,  X_train[:,1].max()+1
# 创建网格
xx, yy = np.meshgrid(
    np.arange(x_min, x_max, 0.02),
    np.arange(y_min, y_max, 0.02),
)
# 展开数据，预测
Z = mlp.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
# 可视化
plt.pcolormesh(xx, yy, Z, alpha=0.2)
plt.scatter(X_train[:,0], X_train[:,1], c = y_train)
```

**hidden_layer_sizes参数**

值是[100,]这意味着模型中只有一个隐藏层，而隐藏层中的节点数是100。如果我们给hidden_layer_sizes定义为[10,10]，那就意味着模型中有两个隐藏层，每层有10个节点

**关于slover参数：**

在机器学习库scikit-learn中的`MLPClassifier`类中，`solver`参数用于指定优化算法，用于训练多层感知器（MLP）模型。`solver`参数的可选值包括：

- "lbfgs"：使用L-BFGS算法，这是一种拟牛顿法，适用于小到中等规模的问题。
- "sgd"：随机梯度下降（Stochastic Gradient Descent），适用于大规模数据集。
- "adam"：一种自适应学习率的优化算法，通常在训练深度学习模型时使用。
- "nesterovs_momentum"：Nesterov加速的SGD，通常可以更快地收敛。

当你设置`solver="lbfgs"`时，你选择了L-BFGS算法作为优化器。L-BFGS是一种高效的优化算法，适用于求解大规模优化问题，特别是当数据集不是非常大时。它是一种基于梯度的优化方法，利用了二阶导数（即Hessian矩阵的近似）来加速收敛。

**关于activation激活函数**

```
activation : {'identity', 'logistic', 'tanh', 'relu'}, default='relu'
    Activation function for the hidden layer.

    - 'identity', no-op activation, useful to implement linear bottleneck,
      returns f(x) = x

    - 'logistic', the logistic sigmoid function,
      returns f(x) = 1 / (1 + exp(-x)).

    - 'tanh', the hyperbolic tan function,
      returns f(x) = tanh(x).

    - 'relu', the rectified linear unit function,
      returns f(x) = max(0, x)
```

查看一下分类的效果，一层隐藏层，10个节点，activation为'relu'，对于分类，除了增加单个隐藏层中的节点数之外，还有两种方法可以让决定边界更细腻：一个是增加隐藏层的数量；另一个是把activation参数改为tanh

![image-20240624175611458](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240624175611458.png)

## 手写数据集（实例）

```python
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import numpy as np

mnist=fetch_openml('mnist_784',version=1,cache=True) # 较慢
# 数据集的格式是70000个手写数据，784个特征
# 784个特征是因为每个图片是28*28个像素点，每个像素点都有一个像素值, 转换成array就是784的长度, 可以通过reshape(28,28)来还原图像
mnist['data'].shape 
# 测试和训练集构建
X = mnist['data']/255
y = mnist.target

X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=5000, test_size=1000, random_state=62)
# 构建一个100个隐藏层, 每个隐藏层100个节点
mlp = MLPClassifier(solver= "lbfgs", 
                    hidden_layer_sizes = [100,100], 
                    activation = 'relu', 
                    alpha = 1e-5, 
                    random_state=62)
mlp.fit(X_train, y_train)
mlp.score(X_test, y_test) # 0.92 拟合度还是很好的

# 自己手写几个数据试试看
from PIL import Image
# 其他的几个识别效果一般吧
image = Image.open('.\\others\\8.jpg').convert('F')
image= image.resize((28,28))

arr = [] # 将28*28的图像转换成长度为784的数组
for i in range(28):
    for j in range(28):
        piex = 1.0 - float(image.getpixel((i,j)))/255.
        arr.append(piex)
#第一个参数是1，表示新的数组应该是一个单行数组；第二个参数是-1，这是一个特殊的参数，它告诉NumPy自动计算这个维度的大小，以便保持数组中元素的总数不变,本例子就是将数组转换为1行784列的数组
arr1 = np.array(arr).reshape(1, -1)
mlp.predict(arr1)
# out: array(['8'], dtype='<U1') # 预测成功
```
# 总结
神经网络可以从超大数据集中获取信息并且可以建立极为复杂的模型，所以在计算能力充足并且参数设置合适的情况下，神经网络可以比其他的机器学习算法表现更加优异。但是它的问题也很突出，如模型训练的时间相对更长、对数据预处理的要求较高等。对于特征类型比较单一的数据集来说，神经网络的表现不错；但如果数据集中的特征类型差异比较大的话，随机森林或是梯度上升随机决策树等基于决策树的算法会表现更好。

另外，神经网络模型中的参数调节也是一门艺术，尤其是隐藏层的数量和隐藏层中节点的数量。对于初学者来说，建议参考这样一个原则，那就是神经网络中隐藏层的节点数约等于训练数据集的特征数量，但是一般不要超过500。在开始训练模型的时候，可以让模型尽量复杂，然后再对正则化参数alpha进行调节来提高模型的表现。

