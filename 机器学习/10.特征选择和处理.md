# 特征处理

## 哑变量

哑变量就是就是一种虚拟变量，就是把某些变量转换为二值变量，pandas中的`get_dummies`可以实现：

```python
import pandas as pd
df = pd.DataFrame(
    {
        "col1":[1,2,3],
        "col2":["a","b", "c"],
        "col3":["A","A","B"]
    }
)
pd.get_dummies(df)
# 默认只对字符列操作, 将数值转为str后就可以应用了
df["col1"] = df['col1'].astype(str)
pd.get_dummies(df)
```

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240628103241073.png" alt="image-20240628103241073" style="zoom:67%;" />

## 装箱

在机器学习中，不同的算法建立的模型会有很大的差别。即便是在同一个数据集中，这种差别也会存在。这是由于算法的工作原理不同所导致的，如KNN和MLP

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPRegressor
from sklearn.neighbors import KNeighborsRegressor

rnd = np.random.RandomState(38)
x = rnd.uniform(-5,5, size = 50)
X = X.reshape(-1,1)
y = ((x*2+2 + rnd.normal(size = len(x))))/2

line = np.linspace(-5,5, 1000, endpoint=False).reshape(-1, 1)
mlp = MLPRegressor().fit(X,y)
knn = KNeighborsRegressor().fit(X,y)

plt.plot(X, y,'o')
plt.plot(line, mlp.predict(line), label = "MLP")
plt.plot(line, knn.predict(line), label = "KNN")
plt.legend()
```

![image-20240628151419050](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240628151419050.png)

MLP产生的回归线非常接近线性模型的结果，而KNN则相对更复杂一些，它试图覆盖更多的数据点。

下面对数据进行离散化处理：

1. 指定容器，我们数据生成的范围是-5，到5，所以容器范围也是
2. 指定容器个数
3. 分箱（将数据转换为所在的区间index）

```python
# 构建
bins = np.linspace(-5,5, 11) #array([-5., -4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.])
# 分箱, 使用np.digitize，可以输出给定数组在给定范围内的index
#Signature: np.digitize(x, bins, right=False)
#Docstring:
#Return the indices of the bins to which each value in input array belongs.
target_bin = np.digitize(X, bins=bins)
target_bin[0:3]
# out
#array([[ 4], -1.1522688属于 -2., -1. 属于第四个区间
#       [9],   3.59707847属于3.,  4 第9个区间
#       [ 10]], dtype=int64)  4.4419963 最后一个区间
```

scikit-learn的独热编码OneHotEncoder。OneHotEncoder和pandas的get_dummies功能基本上是一样的，但是OneHotEncoder目前只能用于整型数值的类型变量

```python
from sklearn.preprocessing import OneHotEncoder
one = OneHotEncoder(sparse=False).fit(target_bin)
X_in_bin = one.transform(target_bin)
```

列的意思就是每个区间(-5,-4), (-4,-3)....(4, 5)，对应的bin的编号

行就是我们生成的X的每个值

相当于我们把原先数据集中的连续特征转化成了类别特征

![image-20240628152608453](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240628152608453.png)



重新进行MLP和KNN

```python
mlp = MLPRegressor().fit(X_in_bin,y)
knn = KNeighborsRegressor().fit(X_in_bin,y)

line = np.linspace(-5,5, 1000, endpoint=False).reshape(-1, 1)
new_line = one.transform(np.digitize(line, bins))

plt.plot(X, y,'o')
plt.plot(line, mlp.predict(new_line), label = "MLP")
plt.plot(line, knn.predict(new_line), label = "KNN")
plt.legend()
```

![image-20240628153640127](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240628153640127.png)

MLP模型和KNN模型变得更相似了，尤其在x>0的部分，两个模型几乎完全重合。MLP的回归模型变得更复杂，而KNN的模型变得更简单。所以这是对样本特征进行装箱的一个好处：它可以纠正模型过拟合或者欠拟合的问题。尤其是当针对大规模高维度的数据集使用线性模型的时候，装箱处理可以大幅提高线性模型的预测准确率

## 升维

数据集的特征不足的情况。要解决这个问题，就需要对数据集的特征进行扩充。这里我们介绍两种在统计建模中常用的方法——交互式特征（Interaction Features）和多项式特征（Polynomial Features），交互式特征是在原始数据特征中添加交互项，使特征数量增加。在Python中，我们可以通过Numpy的hstack函数来对数据集添加交互项

```python
# np.hstack 函数是 NumPy 库中用于水平（横向）堆叠数组的函数。以下是根据提供的文档字符串对该函数的中文解释和使用示例：
#手工生成两个数组
array_1 = [1,2,3,4,5]
array_2 = [6,7,8,9,0]
#使用hstack将两个数组进行堆叠
array_3 = np.hstack((array_1, array_2))
#打印结果
print('将数组2添加到数据1中后得到:{}'.format(array_3)) # 将数组2添加到数据1中后得到:[1 2 3 4 5 6 7 8 9 0]
```

从结果中看到，原来两个5维数组被堆放到一起，形成了一个新的十维数组。也就是说我们使array_1和array_2产生了交互。假如array_1和array_2分别代表两个数据点的特征，那么我们生成的array_3就是它们的交互特征

例如将X.X_in_bin加入

```python
X_multi = np.hstack([X_in_bin, X*X_in_bin])
X_multi[0:4]
```

![image-20240628155551632](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240628155551632.png)

# 特征选择

## 单一变量法进行特征选择

最简单的两种是SelectPercentile和SelectKBest，其中SelectPercentile是自动选择原始特征的百分比，例如原始数据的特征数是200个，那么SelectPercentile的pecentile参数设置为50，就会选择100个原始特征中的50%，即100个，而SelectKBest是自动选择*K*个最重要的特征

### SelectPercentile

导入下面的包，剩下的部分都是基于这些

```python
from sklearn.feature_selection import SelectPercentile, SelectFromModel, RFE
from sklearn.linear_model import Ridge
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
```

使用酒的数据集来验证

```python
data = load_wine()
X = data.data
y = data.target
f_name = data.feature_names
X_train, X_test, y_train, y_test = train_test_split(X, y)

# 不进行特征选择
# 首先标准化数据
ss = StandardScaler()
ss.fit(X_train)
X_train_scale = ss.transform(X_train)
X_test_scale = ss.transform(X_test)

mlpc = MLPClassifier(random_state=60, hidden_layer_sizes = (50,50))
mlpc.fit(X_train_scale, y_train)
mlpc.score(X_test_scale, y_test)
# 模型的得分是 0.97777

# 下面做一下单一变量法的特征选择
select = SelectPercentile(percentile=50)
select.fit(X_train_scale, y_train)
X_train_select = select.transform(X_train_scale)
print(X_train_select.shape) # 
np.array(f_name)[select.get_support()]
```

![image-20240701155024331](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240701155024331.png)

最后保留了6个特征, 测试一下这只用这六个特征对模型的影响

```python
mlpc = MLPClassifier(random_state=60, hidden_layer_sizes = (50,50))
mlpc.fit(X_train_select, y_train)
mlpc.score(select.transform(X_test_scale), y_test) 
# out
# 1.0 模型拟合为1，这个可能是由于数据量太小了...,也能认为模型的预测能力提升了...
# SelectKBest结果一致
select = SelectKBest(k=6)
select.fit(X_train_scale, y_train)
X_train_select = select.transform(X_train_scale)
print(X_train_select.shape)
np.array(f_name)[select.get_support()]
```

### SelectFromModel

基于一个有监督模型的选择，直接从这个属性中抽取特征的重要性。当然除了随机森林之外，其他算法也是可以的，例如使用L1正则化的线性模型，它们可以对数据空间的稀疏系数进行学习，从而可以当作特征重要性来抽取。原本这个系数是线性模型用来为自己建模的，我们也可以借助它来帮助其他模型进行数据预处理，

需要模型中有 ``feature_importances_`` or `coef_`属性，用随机森林和岭回归（L1）来进行一下变量选择

```python
sfm = SelectFromModel(
    RandomForestClassifier(n_estimators=100, random_state=30),
    threshold='median' # 每个变量的feature_importances_ 高于median被保留，反之倍舍弃
    )
sfm.fit(X_train_scale, y_train)
X_train_sfm = sfm.transform(X_train_scale)
print(X_train_sfm.shape)
np.array(f_name)[sfm.get_support()]
```

![image-20240701155817563](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240701155817563.png)

与单一变量的方法基本一致，多了一个特征，是不是二者可以取交集，🤔...

```python
sfr = SelectFromModel(
        Ridge(alpha=1),
    threshold='mean'
        
    )
sfm.fit(X_train_scale, y_train)
X_train_sfm = sfm.transform(X_train_scale)
print(X_train_sfm.shape)
np.array(f_name)[sfm.get_support()]
# 结果和随机森林是一致的
```

### 迭代式选择

迭代式特征选择是基于若干个模型进行特征选择。在scikit-learn中，有一个称为递归特征剔除法（Recurise Feature Elimination, RFE）的功能就是通过这种方式来进行特征选择的。在最开始，RFE会用某个模型对特征进行选择，之后再建立两个模型，其中一个对已经被选择的特征进行筛选；另外一个对被剔除的模型进行筛选，然后一直重复这个步骤，直到达到我们指定的特征数量，需要模型中有 ``feature_importances_`` or `coef_`属性

```python
rfe = RFE(
    estimator=RandomForestClassifier(n_estimators=100, random_state=30),
    n_features_to_select=6
    )
rfe.fit(X_train_scale, y_train)
X_train_rfe = rfe.transform(X_train_scale)
print(X_train_rfe.shape)
np.array(f_name)[rfe.get_support()] # 结果与前面的一样

```

![image-20240701160408761](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240701160408761.png)

本次测试的模型太小，所以效果一般，明白方法就好...
