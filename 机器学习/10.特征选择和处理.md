# 特征选择

## 哑变量

哑变量就是就是一种虚拟变量，就是把某些变量转换为二值变量，pandas中的`get_dummies`可以实现：

```python
import pandas as pd
df = pd.DataFrame(
    {
        "col1":[1,2,3],
        "col2":["a","b", "c"],
        "col3":["A","A","B"]
    }
)
pd.get_dummies(df)
# 默认只对字符列操作, 将数值转为str后就可以应用了
df["col1"] = df['col1'].astype(str)
pd.get_dummies(df)
```

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240628103241073.png" alt="image-20240628103241073" style="zoom:67%;" />

## 装箱

在机器学习中，不同的算法建立的模型会有很大的差别。即便是在同一个数据集中，这种差别也会存在。这是由于算法的工作原理不同所导致的，如KNN和MLP

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.neural_network import MLPRegressor
from sklearn.neighbors import KNeighborsRegressor

rnd = np.random.RandomState(38)
x = rnd.uniform(-5,5, size = 50)
X = X.reshape(-1,1)
y = ((x*2+2 + rnd.normal(size = len(x))))/2

line = np.linspace(-5,5, 1000, endpoint=False).reshape(-1, 1)
mlp = MLPRegressor().fit(X,y)
knn = KNeighborsRegressor().fit(X,y)

plt.plot(X, y,'o')
plt.plot(line, mlp.predict(line), label = "MLP")
plt.plot(line, knn.predict(line), label = "KNN")
plt.legend()
```

![image-20240628151419050](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240628151419050.png)

MLP产生的回归线非常接近线性模型的结果，而KNN则相对更复杂一些，它试图覆盖更多的数据点。

下面对数据进行离散化处理：

1. 指定容器，我们数据生成的范围是-5，到5，所以容器范围也是
2. 指定容器个数
3. 分箱（将数据转换为所在的区间index）

```python
# 构建
bins = np.linspace(-5,5, 11) #array([-5., -4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.,  5.])
# 分箱, 使用np.digitize，可以输出给定数组在给定范围内的index
#Signature: np.digitize(x, bins, right=False)
#Docstring:
#Return the indices of the bins to which each value in input array belongs.
target_bin = np.digitize(X, bins=bins)
target_bin[0:3]
# out
#array([[ 4], -1.1522688属于 -2., -1. 属于第四个区间
#       [9],   3.59707847属于3.,  4 第9个区间
#       [ 10]], dtype=int64)  4.4419963 最后一个区间
```

scikit-learn的独热编码OneHotEncoder。OneHotEncoder和pandas的get_dummies功能基本上是一样的，但是OneHotEncoder目前只能用于整型数值的类型变量

```python
from sklearn.preprocessing import OneHotEncoder
one = OneHotEncoder(sparse=False).fit(target_bin)
X_in_bin = one.transform(target_bin)
```

列的意思就是每个区间(-5,-4), (-4,-3)....(4, 5)，对应的bin的编号

行就是我们生成的X的每个值

相当于我们把原先数据集中的连续特征转化成了类别特征

![image-20240628152608453](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240628152608453.png)



重新进行MLP和KNN

```python
mlp = MLPRegressor().fit(X_in_bin,y)
knn = KNeighborsRegressor().fit(X_in_bin,y)

line = np.linspace(-5,5, 1000, endpoint=False).reshape(-1, 1)
new_line = one.transform(np.digitize(line, bins))

plt.plot(X, y,'o')
plt.plot(line, mlp.predict(new_line), label = "MLP")
plt.plot(line, knn.predict(new_line), label = "KNN")
plt.legend()
```

![image-20240628153640127](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240628153640127.png)

MLP模型和KNN模型变得更相似了，尤其在x>0的部分，两个模型几乎完全重合。MLP的回归模型变得更复杂，而KNN的模型变得更简单。所以这是对样本特征进行装箱的一个好处：它可以纠正模型过拟合或者欠拟合的问题。尤其是当针对大规模高维度的数据集使用线性模型的时候，装箱处理可以大幅提高线性模型的预测准确率

## 升维

数据集的特征不足的情况。要解决这个问题，就需要对数据集的特征进行扩充。这里我们介绍两种在统计建模中常用的方法——交互式特征（Interaction Features）和多项式特征（Polynomial Features），交互式特征是在原始数据特征中添加交互项，使特征数量增加。在Python中，我们可以通过Numpy的hstack函数来对数据集添加交互项

```python
# np.hstack 函数是 NumPy 库中用于水平（横向）堆叠数组的函数。以下是根据提供的文档字符串对该函数的中文解释和使用示例：
#手工生成两个数组
array_1 = [1,2,3,4,5]
array_2 = [6,7,8,9,0]
#使用hstack将两个数组进行堆叠
array_3 = np.hstack((array_1, array_2))
#打印结果
print('将数组2添加到数据1中后得到:{}'.format(array_3)) # 将数组2添加到数据1中后得到:[1 2 3 4 5 6 7 8 9 0]
```

从结果中看到，原来两个5维数组被堆放到一起，形成了一个新的十维数组。也就是说我们使array_1和array_2产生了交互。假如array_1和array_2分别代表两个数据点的特征，那么我们生成的array_3就是它们的交互特征

例如将X.X_in_bin加入

```python
X_multi = np.hstack([X_in_bin, X*X_in_bin])
X_multi[0:4]
```

![image-20240628155551632](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240628155551632.png)





