# 决策树和随机森林

决策树是一种在分类与回归中都有非常广泛应用的算法，它的原理是通过对一系列问题进行if/else的推导，最终实现决策

## 使用sklearn实现决策树

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import tree, datasets
from sklearn.model_selection import train_test_split

# 使用前两个特征
X = datasets.load_wine().data[:,:2]
y = datasets.load_wine().target
X_train, X_test, y_train, y_test = train_test_split(X, y)
# 构建决策树
clf = tree.DecisionTreeClassifier(max_depth=3)
clf.fit(X_train, y_train)
```

可以通过可视化看一下分类效果，仅适用于feature数小于3

```python
# 查看分类效果
x_min, x_max = X_train[:,0].min()-1,  X_train[:,0].max()+1
y_min, y_max = X_train[:,1].min()-1,  X_train[:,1].max()+1
# 创建网格
xx, yy = np.meshgrid(
    np.arange(x_min, x_max, 0.02),
    np.arange(y_min, y_max, 0.02),
)
# 展开数据，预测
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
# 可视化
plt.pcolormesh(xx, yy, Z, alpha=0.2)
plt.scatter(X_train[:,0], X_train[:,1], c = y_train)
```

![image-20240619162841691](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240619162841691.png)

数据被分成了三类，大趋势是正确的，还是有不少的点分类错误，尝试提高max_depth=5

```python
clf = tree.DecisionTreeClassifier(max_depth=5)
clf.fit(X_train, y_train)
....some code
```

![image-20240619163031006](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240619163031006.png)

可以看出分类要比之前的好很多，看看准确度怎么样：

```python
clf.score(X_test, y_test)
# out
# 0.73333
```

一般般吧

相比其他算法，决策树有一个非常大的优势，**就是可以很容易地将模型进行可视化**。这样就可以让非专业人士也可以看得明白。另外，由于决策树算法对每个样本特征进行单独处理，因此并**不需要对数据进行转换**，但是决策树可能出现过拟合现象

原因：

决策树虽然考虑到了各个特征，但是会忽略特征间的关系；异常值也可能影响判断；属于稀疏矩阵（scRNA的表达矩阵）；可能混进去一些奇奇怪怪的特征（比如一些编号之类的，但对别的模型来说这一点是一样的（笑）

## 决策树的可视化

使用graphviz包可视化决策树

安装方法：

1. pip install graphviz

2. 下载安装Graphviz:

   ```
   https://graphviz.org/download/
   ```

3. 将Graphviz命令添加进环境变量：

   ```
   直接修改win系统设置，或者
   import os
   os.environ['PATH'] = os.environ['PATH']+ ';C:\\Program Files\\Graphviz\\bin'
   ```

4. 使用

   ```python
   import graphviz
   from sklearn.tree import export_graphviz
   graphviz.Source(
   	export_graphviz(
       	clf,out_file=None, feature_names=datasets.load_wine().feature_names[:2],
       	class_names=datasets.load_wine().target_names, impurity=False, filled=True
   	)
   )
   '''
   先从决策树的根部开始看起，第一个条件是酒精含量小于或等于12.745，samples = 133指在根节点上，有133个样本。Value =[41, 53, 39]是指有41个样本属于class_0，53个样本属于class_1，其余39个样本属于class_2。接下来我们跟着树枝一起前进，在酒精度小于或等于12.745这个条件为True的情况下，决策树判断分类为class_1，如果是False，则判断为class_0，这样到下一层，判断为class_1的样本共有53个，而判断为class_0的样本则有80个，而再下一层则对酒的苹果酸含量进行判断，进一步对样本进行分类。左边class_1的分支的判断条件是苹果酸含量小于或等于2.445，如果为True，则再判断酒精含量是否小于或等于12.49，如果为False则判断酒精含量是否低于12.12，依此类推，直到将样本全部放进3个分类当中
   '''
   ```

![image-20240619164400046](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240619164400046.png)

## 随机森林

随机森林就是将许多的决策树放在一起去构建模型，对每个预测的结果取均值...这样对于降低过拟合效果较好和此相似的集合模型的算法还有梯度上升决策树（Gradient Boosted Decision Trees, GBDT）

## 使用sk-learn实现随机森林

```python
from sklearn.ensemble import RandomForestClassifier
import numpy as np
import matplotlib.pyplot as plt
from sklearn import tree, datasets
from sklearn.model_selection import train_test_split

X = datasets.load_wine().data[:,:2]
y = datasets.load_wine().target
X_train, X_test, y_train, y_test = train_test_split(X, y)

clf = RandomForestClassifier(n_estimators=3, bootstrap=False)
clf.fit(X_train, y_train)
# 查看分类效果
x_min, x_max = X_train[:,0].min()-1,  X_train[:,0].max()+1
y_min, y_max = X_train[:,1].min()-1,  X_train[:,1].max()+1
# 创建网格
xx, yy = np.meshgrid(
    np.arange(x_min, x_max, 0.02),
    np.arange(y_min, y_max, 0.02),
)
# 展开数据，预测
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.pcolormesh(xx, yy, Z, alpha=0.2)
plt.scatter(X_train[:,0], X_train[:,1], c = y_train)
```

![image-20240619172637662](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240619172637662.png)

可以看出数据点基本都被分到了正确的位置，相对于决策树要好不少

## 随机森林的几个重要参数

`n_estimators`: 生成的决策树的数量，一般多了比较好，太多可能出现过拟合

```
每棵树预测出数据类别的概率，比如其中一棵树说，“这瓶酒80%属于class_1”，另外一棵树说，“这瓶酒60%属于class_2”，随机森林会把这些概率取平均值，然后把样本放入概率最高的分类当中
```

`bootstrap`: 是否有放回的抽样，默认True，即有放回抽样，原因是因为通过重新生成数据集，可以让随机森林中的每一棵决策树在构建的时候，会彼此之间有些差异。再加上每棵树的节点都会去选择不同的样本特征，经过这两步动作之后，可以完全肯定随机森林中的每棵树都不一样，这也符合我们使用随机森林的初衷

`max_features`: 使用的feature的数量，

    - If "auto", then `max_features=sqrt(n_features)`.
    - If "sqrt", then `max_features=sqrt(n_features)`.
    - If "log2", then `max_features=log2(n_features)`.
    - If None, then `max_features=n_features`. # 使用全部 不建议
    假如把max_features设置为样本全部的特征数n_features就意味着模型会在全部特征中进行筛选，这样在特征选择这一步，就没有随机性可言了。而如果把max_features的值设为1，就意味着模型在数据特征上完全没有选择的余地，只能去寻找这1个被随机选出来的特征向量的阈值了。所以说，max_features的取值越高，随机森林里的每一棵决策树就会“长得更像”，它们因为有更多的不同特征可以选择，也就会更容易拟合数据；反之，如果max_features取值越低，就会迫使每棵决策树的样子更加不同，而且因为特征太少，决策树们不得不制造更多节点来拟合数据

`random_state`: 这个要固定，否则每次的结果可能会差的比较多

## 关于决策树和随机森林的过拟合比较

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import tree, datasets
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier

X = datasets.load_wine().data
y = datasets.load_wine().target
X_train, X_test, y_train, y_test = train_test_split(X, y)
clf = tree.DecisionTreeClassifier(max_depth=3,random_state=8)
clf.fit(X_train, y_train)
clf2 = RandomForestClassifier(max_depth=3, random_state=8)
clf2.fit(X_train, y_train)
# 决策树
print(clf.score(X_train, y_train))
print(clf.score(X_test, y_test))
# 0.9924812030075187
# 0.8666666666666667
# 随机森林
print(clf2.score(X_train, y_train))
print(clf2.score(X_test, y_test))
#0.9924812030075187
#0.9777777777777777
```

可以看出随机森林的过拟合现象要远远好于决策树

## 变量重要度排序

```python
import pandas as pd
df = pd.DataFrame(
    {
        "feature":datasets.load_wine().feature_names,
        "importance":clf2.feature_importances_
    }
).sort_values(by = "importance")

df.plot.bar(x='feature', y='importance')
```

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240619175635923.png" alt="image-20240619175635923" style="zoom:80%;" />

## 总结

对于超高维数据集、稀疏数据集等来说，随机森林就有点捉襟见肘了，在这种情况下，线性模型要比随机森林的表现更好一些。还有，随机森林相对更消耗内存，速度也比线性模型要慢，所以如果程序希望更节省内存和时间的话，建议还是选择线性模型。

## 随机森林的实例

下载adult数据集：

https://archive.ics.uci.edu/dataset/2/adult

解压后直接读取adult.data(csv文件)即可

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt

df = pd.read_csv(".\\机器学习\\adult\\adult.data", header=None)
df.columns = ["年龄", "单位性质", "权重", "学历", 
              "受教育时长", "婚姻状况", "职业", "家庭状况",
             "种族", "性别", "资产所得", "资产损失", "周工作时长",'原籍', '收入']
# 选取一部分数据
df = df[["年龄", "单位性质", "学历", "性别", "周工作时长",'职业', '收入']]
# 使用get_dummies构建数据
df_use = get_dummies(df)
```

`get_dummies` 是一个在 Python 的 `pandas` 库中用于数据编码的函数，它可以将分类变量（也称为名义变量）转换为哑变量（dummy variables），这是一种适合机器学习算法的数值表示方法。

就是将分类变量中每个变量转换为0-1矩阵，以适应机器学习的输入

![image-20240620161543784](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240620161543784.png)

```python
# 构建数据集特征
# 我们取最后一列的收入>50k作为label, 大于50k-1， 小于50k-0
X = pd.get_dummies(df).iloc[:,:-2].values
y = pd.get_dummies(df).loc[:,"收入_ >50K"].values
# 构建测试与训练数据
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=8888)
clf = RandomForestClassifier()
clf.fit(X_train, y_train)
clf.score(X_train, y_train) # 0.94
clf.score(X_test, y_test) # 0.79
```

