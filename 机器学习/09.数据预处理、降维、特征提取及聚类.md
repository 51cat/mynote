# 数据预处理、降维、特征提取及聚类

## 数据预处理

### StandardScaler

对数据进行z值标准化，转换为均值为0标准差为1的数据，值的意义变为高于（>0）或者低于（<0）均值几个标准差，并不会改变数据的分布，可以确保数据的“大小”都是一致的，这样更利于模型的训练

![image-20240626145940762](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240626145940762.png)

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler

X, y = make_blobs(n_samples=40, centers=2, random_state=50, cluster_std=2)
plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.cool)

X_1 = StandardScaler().fit(X).transform(X)
plt.scatter(X_1[:,0], X_1[:,1], c=y, cmap=plt.cm.cool)
```

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240626150252508.png" alt="image-20240626150252508" style="zoom:50%;" />

### MinMaxScaler

本方法可以将数据缩放到0-1之间

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240626150457310.png" alt="image-20240626150457310" style="zoom:80%;" />

其本身也不会改变数据的分布

```python
...
MinMaxScaler().fit(X).transform(X)
```

### RobustScaler


RobustScaler是scikit-learn库中提供的一种特征缩放方法，它与MinMaxScaler或StandardScaler不同，它使用的是数据的中位数（median）和四分位数范围（interquartile range, IQR）来进行缩放，这使得RobustScaler对异常值（outliers）具有较好的鲁棒性。

- <img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240626151116407.png" alt="image-20240626151116407" style="zoom:67%;" />

使用RobustScaler时，数据首先会被中心化（减去中位数），然后除以IQR进行缩放。这种缩放方式可以有效地减少异常值对缩放结果的影响。

```python
from sklearn.preprocessing import RobustScaler
X_2 = RobustScaler().fit(X).transform(X)
```

### Normalizer

这种方法将所有样本的特征向量转化为欧几里得距离为1。也就是说，它把数据的分布变成一个半径为1的圆，或者是一个球。Normalizer通常是在我们只想保留数据特征向量的方向，而忽略其数值的时候使用(类似于投影？把数据点投到半径为1的圆上)

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240626151758921.png" alt="image-20240626151758921" style="zoom: 67%;" />

```python
from sklearn.preprocessing import Normalizer
X_2 = Normalizer().fit(X).transform(X)
plt.scatter(X_2[:,0], X_2[:,1], c=y, cmap=plt.cm.cool)
plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.cool)
```

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240626151553458.png" alt="image-20240626151553458" style="zoom:50%;" />

数据预处理主要针对数值敏感的模型，具体见前面的文档...

## 降维

有些数据集的维度会达到上千甚至上万，这样如果不进行数据降维操作的话，对于机器学习模型来说，处理的过程可能会非常缓慢。另外，还会有一些特征之间有非常强烈的相关性，比如人口数据集中，如果性别为男这一列取值为1，则性别为女这一列取值只能是0，去掉其中任何一列不会丢失任何信息，在这种情况下，我们就会进行降维，以便降低模型的复杂度

### 使用PCA分析降维

```python
from sklearn.decomposition import PCA
X, y = make_blobs(n_samples=40, centers=2, random_state=50, cluster_std=2)
pca = PCA(n_components=2) # 表示选取前两个主成分, 如果是一个介于0,1之间的数字，例如0.9则会选取可以表示原始特征90%的主成分
pca.fit(X)
X_pca = pca.transform(X) # 提去转换后的数据
pca.explained_variance_ratio_ # 查看两个主成分解释了多少变量, 因为原始的数据是二维的所以两个主成分的和就是1
```

用酒数据集做一个PCA

```python
from sklearn.datasets import load_wine
from sklearn.decomposition import PCA

wine = load_wine()
X = wine.data
y = wine.target
X_scale = StandardScaler().fit(X).transform(X) # 要计算欧氏距离：feature量纲不同pca必须要标准化
pca = PCA(n_components=2) # 选取前两个主成分
pca.fit(X_scale)
X_pca = pca.transform(X_scale)
pca.explained_variance_ratio_
np.sum(pca.explained_variance_ratio_) #0.554 前两个主成分解释了55.4%的变量

plt.scatter(X_pca[:,0][wine.target==0], X_pca[:,1][wine.target==0])
plt.scatter(X_pca[:,0][wine.target==1], X_pca[:,1][wine.target==1])
plt.scatter(X_pca[:,0][wine.target==2], X_pca[:,1][wine.target==2])
plt.legend(wine.target_names)
```

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240626161049141.png" alt="image-20240626161049141" style="zoom:67%;" />

## 特征提取

PCA分析除了降维，可以看数据间潜在的变化趋势以及低维度的分布情况，用其进行特征提取也是一个重要的方向

使用一个相对复杂一点的数据集——LFW人脸识别数据集进行特征提取

```python
from sklearn.datasets import fetch_lfw_people
lfw = fetch_lfw_people(min_faces_per_person=20, resize=0.8)
```

### 直接神经网络构建模型

