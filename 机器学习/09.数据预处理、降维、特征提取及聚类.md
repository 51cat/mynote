# 数据预处理、降维、特征提取及聚类

## 数据预处理

### StandardScaler

对数据进行z值标准化，转换为均值为0标准差为1的数据，值的意义变为高于（>0）或者低于（<0）均值几个标准差，并不会改变数据的分布，可以确保数据的“大小”都是一致的，这样更利于模型的训练

![image-20240626145940762](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240626145940762.png)

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler

X, y = make_blobs(n_samples=40, centers=2, random_state=50, cluster_std=2)
plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.cool)

X_1 = StandardScaler().fit(X).transform(X)
plt.scatter(X_1[:,0], X_1[:,1], c=y, cmap=plt.cm.cool)
```

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240626150252508.png" alt="image-20240626150252508" style="zoom:50%;" />

### MinMaxScaler

本方法可以将数据缩放到0-1之间

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240626150457310.png" alt="image-20240626150457310" style="zoom:80%;" />

其本身也不会改变数据的分布

```python
...
MinMaxScaler().fit(X).transform(X)
```

### RobustScaler


RobustScaler是scikit-learn库中提供的一种特征缩放方法，它与MinMaxScaler或StandardScaler不同，它使用的是数据的中位数（median）和四分位数范围（interquartile range, IQR）来进行缩放，这使得RobustScaler对异常值（outliers）具有较好的鲁棒性。

- <img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240626151116407.png" alt="image-20240626151116407" style="zoom:67%;" />

使用RobustScaler时，数据首先会被中心化（减去中位数），然后除以IQR进行缩放。这种缩放方式可以有效地减少异常值对缩放结果的影响。

```python
from sklearn.preprocessing import RobustScaler
X_2 = RobustScaler().fit(X).transform(X)
```

### Normalizer

这种方法将所有样本的特征向量转化为欧几里得距离为1。也就是说，它把数据的分布变成一个半径为1的圆，或者是一个球。Normalizer通常是在我们只想保留数据特征向量的方向，而忽略其数值的时候使用(类似于投影？把数据点投到半径为1的圆上)

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240626151758921.png" alt="image-20240626151758921" style="zoom: 67%;" />

```python
from sklearn.preprocessing import Normalizer
X_2 = Normalizer().fit(X).transform(X)
plt.scatter(X_2[:,0], X_2[:,1], c=y, cmap=plt.cm.cool)
plt.scatter(X[:,0], X[:,1], c=y, cmap=plt.cm.cool)
```

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240626151553458.png" alt="image-20240626151553458" style="zoom:50%;" />

数据预处理主要针对数值敏感的模型，具体见前面的文档...

## 降维

有些数据集的维度会达到上千甚至上万，这样如果不进行数据降维操作的话，对于机器学习模型来说，处理的过程可能会非常缓慢。另外，还会有一些特征之间有非常强烈的相关性，比如人口数据集中，如果性别为男这一列取值为1，则性别为女这一列取值只能是0，去掉其中任何一列不会丢失任何信息，在这种情况下，我们就会进行降维，以便降低模型的复杂度

### 使用PCA分析降维

```python
from sklearn.decomposition import PCA
X, y = make_blobs(n_samples=40, centers=2, random_state=50, cluster_std=2)
pca = PCA(n_components=2) # 表示选取前两个主成分, 如果是一个介于0,1之间的数字，例如0.9则会选取可以表示原始特征90%的主成分
pca.fit(X)
X_pca = pca.transform(X) # 提去转换后的数据
pca.explained_variance_ratio_ # 查看两个主成分解释了多少变量, 因为原始的数据是二维的所以两个主成分的和就是1
```

用酒数据集做一个PCA

```python
from sklearn.datasets import load_wine
from sklearn.decomposition import PCA

wine = load_wine()
X = wine.data
y = wine.target
X_scale = StandardScaler().fit(X).transform(X) # 要计算欧氏距离：feature量纲不同pca必须要标准化
pca = PCA(n_components=2) # 选取前两个主成分
pca.fit(X_scale)
X_pca = pca.transform(X_scale)
pca.explained_variance_ratio_
np.sum(pca.explained_variance_ratio_) #0.554 前两个主成分解释了55.4%的变量

plt.scatter(X_pca[:,0][wine.target==0], X_pca[:,1][wine.target==0])
plt.scatter(X_pca[:,0][wine.target==1], X_pca[:,1][wine.target==1])
plt.scatter(X_pca[:,0][wine.target==2], X_pca[:,1][wine.target==2])
plt.legend(wine.target_names)
```

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240626161049141.png" alt="image-20240626161049141" style="zoom:67%;" />

## 特征提取

PCA分析除了降维，可以看数据间潜在的变化趋势以及低维度的分布情况，用其进行特征提取也是一个重要的方向

使用一个相对复杂一点的数据集——LFW人脸识别数据集进行特征提取

```python
from sklearn.datasets import fetch_lfw_people
lfw = fetch_lfw_people(min_faces_per_person=20, resize=0.8)
```

### 直接神经网络构建模型

pass

## 聚类

### k-means

K-means聚类用于将数据点分组成K个簇（clusters）。它的目标是使得每个簇内的点尽可能相似（即紧密聚集在一起），同时不同簇之间的点尽可能不同（即簇间距离尽可能远）。下面是K-means聚类的通俗解释和基本步骤：

1. **选择K值**：

   - K是你想要分出的簇的数量。这个值可以基于问题的需求或通过肘部法则（Elbow Method）等方法来辅助决定。

2. **初始化中心点**：

   - 随机选择K个数据点作为初始的簇中心

3. **分配数据点**：

   - 将每个数据点分配到最近的簇中心，形成K个簇。这里的“最近”通常是指欧氏距离最近。

4. **重新计算中心点**：

   - 对于每个簇，计算簇内所有点的质心，然后将这个点作为新的簇中心。

     ![image-20240627164710218](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240627164710218.png)

5. **迭代优化**：

   - 重复步骤3和4，直到簇中心不再显著变化，或者达到预设的迭代次数。

6. **聚类完成**：

   - 当算法收敛，即连续迭代中簇中心的变化很小或为零时，聚类过程完成。

K-means聚类的优点包括简单易懂、计算效率高，适用于大规模数据集。但它也有一些局限性，比如对初始中心点敏感（可能导致局部最优解），以及需要预先指定K值

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.datasets import make_blobs

blobs = make_blobs(random_state=1, centers=1)
x_blobs = blobs[0]  # 只要feature 不要label


# 要求KMeans将数据聚为3类
kmeans = KMeans(n_clusters=3)

# 拟合数据
kmeans.fit(x_blobs)

# 用来画图的代码
x_min, x_max = x_blobs[:, 0].min() - 0.5, x_blobs[:, 0].max() + 0.5
y_min, y_max = x_blobs[:, 1].min() - 0.5, x_blobs[:, 1].max() + 0.5
xx, yy = np.meshgrid(np.arange(x_min, x_max, .02),
                     np.arange(y_min, y_max, .02))

# 预测网格上每个点的簇分配
z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])
Z = z.reshape(xx.shape)

# 显示图像
plt.figure(1)
plt.clf()  # 清除当前图像框架
plt.imshow(Z, interpolation='nearest',
           extent=(x_min, x_max, y_min, y_max),
           cmap=plt.cm.summer,
           aspect='auto', origin='lower')

# 绘制数据点
plt.plot(x_blobs[:, 0], x_blobs[:, 1], 'r.', markersize=5)

# 用蓝色叉号代表聚类的中心
centroids = kmeans.cluster_centers_
plt.scatter(centroids[:, 0], centroids[:, 1],
            marker='x', s=150, linewidths=3,
            color='b', zorder=10)  # 假设 zorder=10 用于确保中心点在图像的最上层

# 设置图像的x和y轴范围
plt.xlim(x_min, x_max)
plt.ylim(y_min, y_max)

# 不显示x和y轴的刻度
plt.xticks(())
plt.yticks(())

# 显示图像
plt.show()
# 查看label
kmeans.labels_
```

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240627165319663.png" alt="image-20240627165319663" style="zoom:67%;" />

可以看出来被成功分成了三类（n_clusters=3），*K*均值算法十分简单而且容易理解，但它也有很明显的局限性。例如，它认为每个数据点到聚类中心的方向都是同等重要的。这样一来，对于“形状”复杂的数据集来说，*K*均值算法就不能很好地工作

### 凝聚聚类

就是层次聚类，通过计算样本之间的距离进行聚类基本步骤和k-means有些像，会输出一个聚类树

1. **初始化**：每个数据点最初被视为一个单独的簇。
2. **寻找最近簇对**：在所有簇对之间找到距离最近的一对。
3. **合并簇**：将距离最近的簇对合并为一个新的簇。
4. **更新距离矩阵**：重新计算新簇与所有其他簇之间的距离。
5. **重复**：重复步骤2-4，直到所有数据点合并成一个包含所有数据点的单个簇

似乎用R更简单 dist() -> hclust()
```r
the agglomeration method to be used. This should be (an unambiguous abbreviation of) one of "ward.D", "ward.D2", "single", "complete", "average" (= UPGMA), "mcquitty" (= WPGMA), "median" (= WPGMC) or "centroid" (= UPGMC).
```

python的不如r方便......

```python
import numpy as np
import matplotlib.pyplot as plt
from scipy.cluster.hierarchy import dendrogram, linkage
from scipy.cluster.hierarchy import fcluster

# 准备数据
np.random.seed(0)
X = np.random.randn(10, 10)  # 10个样本，每个样本10个特征

# 计算凝聚聚类
Z = linkage(X, 'ward')  # 使用Ward's方法

# 绘制树状图
plt.figure(figsize=(10, 7))
dendrogram(Z, orientation='top', distance_sort='descending', show_leaf_counts=True)
plt.title('Hierarchical Clustering Dendrogram')
plt.xlabel('Sample index')
plt.ylabel('Distance')
plt.show()

# 确定簇的数量并提取簇
t = 3  # 阈值可以根据数据和需求调整
clusters = fcluster(Z, t, criterion='distance')
print("簇的分配：", clusters)
```

### DBSCAN算法

基于密度的有噪声应用空间聚类”（Density-based spatial clustering of applications with noise）。这是一个很长且拗口的名字，但是也反应了它的工作原理。DBSCAN是通过对特征空间内的密度进行检测，密度大的地方它会认为是一个类，而密度相对小的地方它会认为是一个分界线。

```python
from sklearn.cluster import DBSCAN

x_blobs = make_blobs( random_state=1, centers=2)[0]
db = DBSCAN()
clus = db.fit_predict(x_blobs)
plt.scatter(x_blobs[:,0], x_blobs[:,1], c = clus,)
```

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240627172115263.png" alt="image-20240627172115263" style="zoom:80%;" />

```python
clus
array([-1, -1, -1,  0, -1,  1,  1,  1, -1,  0,  2,  0,  1, -1, -1,  1,  1,
        0,  0,  3,  1,  1,  1,  0,  0, -1,  1,  0, -1,  2,  0,  0, -1,  0,
        1,  1, -1,  3,  1,  1,  1,  2,  0,  3,  0,  1, -1, -1,  0,  0,  0,
        2,  1, -1,  0,  1,  0, -1,  0,  0,  0, -1, -1,  1, -1, -1, -1,  3,
        0,  0,  1, -1,  1, -1, -1, -1,  1,  1,  0,  0, -1,  3,  0, -1,  0,
       -1, -1, -1,  0,  0,  1,  0,  0,  0,  1, -1,  2,  0,  1, -1],
      dtype=int64)
```

-1代表该数据点是噪声（颜色最深的点）。中间深色的数据点密度相对较大，因此DBSCAN把它们归到一“坨”，而外围的浅色的数据点，DBSCAN认为根本不属于任何一类，所以放进了“噪声”这个类别

DBSCAN中两个非常重要的参数：一是eps；一个是min_samples。eps指定的是考虑划入同一“坨”的样本距离有多远，eps值设置得越大，则聚类所覆盖的数据点越多，反之则越少。默认情况下eps的值为0.5

```pyth
x_blobs = make_blobs( random_state=1, centers=2)[0]
db = DBSCAN(eps=1)
clus = db.fit_predict(x_blobs)
plt.scatter(x_blobs[:,0], x_blobs[:,1], c = clus )
```

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240627172511537.png" alt="image-20240627172511537" style="zoom:67%;" />

esp=1更多的点被放了进来，噪声明显减少

min_samples参数指定的是在某个数据点周围，被看成是聚类核心点的个数，min_samples值越大，则核心数据点越少，噪声也就越多；反之min_sample值越小，噪声也就越少。默认的min_samples值是2

```python
x_blobs = make_blobs( random_state=1, centers=2)[0]
db = DBSCAN(min_samples=10)
clus = db.fit_predict(x_blobs)    
```

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240627172919631.png" alt="image-20240627172919631" style="zoom:67%;" />

通过对eps和min_samples参数赋值，相当于间接地指定了clusters的数量。尤其是eps参数尤为重要,将数据集先用MinMaxScaler或者StandardScaler进行预处理，那么DBSCAN算法的表现会更好

## 优缺点

### DBSCAN（基于密度的聚类算法）

**优点**：

1. **对噪声点具有鲁棒性**：DBSCAN可以很好地处理包含异常值或噪声的数据集。
2. **发现任意形状的簇**：与K-Means不同，DBSCAN不需要事先指定簇的数量，并且可以发现任意形状的簇。
3. **无需指定簇的数量**：算法自动发现簇的数量。

**缺点**：

1. **对参数敏感**：DBSCAN的性能高度依赖于`eps`（邻域半径）和`min_samples`（一个区域内的最小样本数）的设置。
2. **计算复杂度较高**：对于大型数据集，DBSCAN可能会比较慢，因为它需要计算每个点的邻域。
3. **难以处理不同密度的簇**：如果簇的密度差异很大，DBSCAN可能无法准确地识别所有簇。

### K-Means（K均值聚类）

**优点**：

1. **简单易懂**：K-Means算法易于理解和实现。
2. **计算效率高**：对于大型数据集，K-Means通常比其他聚类算法更快。
3. **可扩展性好**：K-Means算法可以很容易地应用于大规模数据集。

**缺点**：

1. **对初始中心点敏感**：K-Means的性能可能因初始中心点的选择而有所不同，可能导致局部最优解。
2. **需要预先指定K值**：用户需要预先指定簇的数量，这在实际应用中可能并不容易确定。
3. **假设簇是凸形和相似大小**：K-Means假设簇是凸形的，并且簇的大小大致相同，这在现实世界的数据中并不总是成立。

### 层次聚类（Hierarchical Clustering）

**优点**：

1. **不需要预先指定簇的数量**：层次聚类不需要用户指定簇的数量，可以生成一个聚类树（树状图）供进一步分析。
2. **灵活的数据表示**：**层次聚类可以生成不同层次的聚类结果，提供数据的多尺度视图。**
3. **发现不同密度的簇**：特别是使用Ward's方法时，层次聚类可以较好地处理不同密度的簇。

**缺点**：

1. **计算复杂度较高**：尤其是对于大型数据集，层次聚类可能需要较长的处理时间。
2. **结果难以解释**：生成的树状图可能难以解释，特别是当簇的数量很多时。
3. **合并过程不可逆**：一旦两个簇合并，就无法撤销，这可能导致聚类结果的偏差。

总结来说，选择哪种聚类算法取决于具体的数据特性和分析目标。每种算法都有其适用场景和限制，理解这些优缺点有助于在实际应用中做出更合适的选择。
