# K-最近邻算法

k近邻应该是机器学习里最简单的算法了

## 基本原理

分类：比如在一个分类任务中，我们要确定一个点的颜色是浅色还是深色，比如我们将k设置为1，那么距离这个点最近的一个点的颜色就被认为是该点的颜色，k=1一般不会用，因为距离最近的点不一定就是正确的，所以一般会k>1, 比如当K=10，那么距离这个点最近的top10（多采用欧氏距离）的点的颜色中出现次数最多的那个颜色就被认为是该点的颜色

欧氏距离：
$$
d(\mathbf{x}, \mathbf{y}) = \sqrt{\sum_{i=1}^{n} (x_i - y_i)^2}
$$
回归：回归与分类类似，当k=1则将距离最近的那个点的值认为是未知点的值，k>1 比如k=10，则是取距离最近的前10个点的值的均值作为未知点的值

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240607151535249.png" alt="image-20240607151535249" style="zoom:80%;" />

> 在scikit-learn中，K最近邻算法的K值是通过n_neighbors参数来调节的，默认值是5

## 应用

步骤：数据读取/构建 -> 构建KNN分类器 -> 预测

### 一个简单的例子

二分类的例子

1. 导入包和构建分类器

```python
from sklearn.datasets import make_blobs
# make_blobs 函数用于生成随机的多维数据集，这些数据集可以被用来作为聚类分析算法的测试数据。
# make_blobs 还可以生成带有异常值的数据集，
# 通过设置 cluster_std（控制簇内样本的方差）和 center_box（控制中心点在特征空间中的位置）等参数来实现

from matplotlib import pyplot as plt
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import train_test_split
import numpy as np

## 构建实例数据一个分类器
# x -> 值
# y -> label
x, y = make_blobs(n_samples=200, centers=2, random_state=8)
# 构建KNN实例
clf = KNeighborsClassifier(n_neighbors=10)
clf.fit(x,y)
```

2. 可视觉化分类情况

```python
plt.scatter(x[:,0], x[:,1], c = y)
# 绘制个目标点
plt.scatter(6.75,10.82, marker='*', c='red', s=200)
plt.scatter(6.75,4.82, marker='*', c='red', s=200)

# 绘制分类范围
# 创建了一个网格，
# 并使用分类器对这个网格中的每个点进行预测，最后使用pcolormesh函数将分类结果绘制出来，形成了一个分类区域的可视化图
x_min, x_max  = x[:,0].min(), x[:,0].max()
y_min, y_max = x[:,1].min(), x[:,1].max()

# meshgrid函数是NumPy库中的一个函数，用于生成网格点
xx, yy = np.meshgrid(
    np.arange(x_min, x_max, .02),
    np.arange(y_min, y_max, .02)
    )
# np.c_[] 见关于meshgrid的用法.md
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) # .ravel() 将多维数组转换为一维数组 
Z = Z.reshape(xx.shape)
plt.pcolormesh(xx, yy, Z,alpha=0.3)
```

![image-20240607161753627](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240607161753627.png)

3. 预测：

```python
print('新数据点的分类是：',clf.predict([[6.75,10.82]]))
print('新数据点的分类是：',clf.predict([[6.75,4.82]]))
# out
# 新数据点的分类是： [0]
# 新数据点的分类是： [1]
```

### K最近邻算法处理多元分类任务

```python
## 构建一个更大的数据集
x, y = make_blobs(n_samples=500, centers=5, random_state=8)
# 构建KNN实例
clf = KNeighborsClassifier(n_neighbors=10)
clf.fit(x,y)
plt.scatter(x[:,0], x[:,1], c = y)

x_min, x_max  = x[:,0].min(), x[:,0].max()
y_min, y_max = x[:,1].min(), x[:,1].max()
# meshgrid函数是NumPy库中的一个函数，用于生成网格点
xx, yy = np.meshgrid(
    np.arange(x_min, x_max, .02),
    np.arange(y_min, y_max, .02)
    )
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])
Z = Z.reshape(xx.shape)
plt.pcolormesh(xx, yy, Z,alpha=0.3) # 用于展示决策边界, xx, yy 为网格点，Z是每个点预测的lable
```

![image-20240607172841265](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240607172841265.png)

可以看出上面有部分点分类不正确

```python
# 看一下正确率
print('模型正确率：{:.2f}'.format(clf.score(x,y)))
# 0.95
```

## K最近邻回归

使用随机数据进行一个测试：

```python
from sklearn.datasets import make_regression
from matplotlib import pyplot as plt
from sklearn.neighbors import KNeighborsRegressor
import numpy as np

# 构建fake data
X, Y = make_regression(n_features=1, n_informative=1, noise=50, random_state=8)
# 构建预测器
reg = KNeighborsRegressor()
reg.fit(X, Y)
# 预测
z = np.linspace(-3, 3, 200).reshape(-1,1) #reshape(-1, 1) 将数组重塑为两维列向量。如果原始数组有200个元素，重塑后的数组将有200行1列。
plt.scatter(x=X, y = Y,c = "orange")
plt.plot(z, reg.predict(z), c = 'k')
```

黑色的线是预测值的连线，似乎并未将所有的点纳入考虑

![image-20240612150219083](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240612150219083.png)

### 评分

调用score方法对模型进行评分 (计算R2)

```python
# 对模型评分
'''
Return the coefficient of determination of the prediction.

The coefficient of determination :math:`R^2` is defined as
:math:`(1 - \frac{u}{v})`, where :math:`u` is the residual
sum of squares ``((y_true - y_pred)** 2).sum()`` and :math:`v`
is the total sum of squares ``((y_true - y_true.mean()) ** 2).sum()``.
The best possible score is 1.0 and it can be negative (because the
model can be arbitrarily worse). A constant model that always predicts
the expected value of `y`, disregarding the input features, would get
a :math:`R^2` score of 0.0.
r2计算，1-残差平方和除以总方差平方和
计算 R^2 值的组成部分
u = np.sum((y_true - y_pred) ** 2)
v = np.sum((y_true - y_true_mean) ** 2)

# 计算 R^2 值
r_squared = 1 - (u / v)
'''
reg.score(X,Y)
# out
# 0.7721167832505298
```

模型的评分为0.77，较低，通过将尝试将n_neighbor设定为一个较小的值会改善，但是R2过于高可能会过拟合..

```python
# 尝试减少n_neighbor
reg = KNeighborsRegressor(n_neighbors=2) # 不会过拟合么??
reg.fit(X, Y)
reg.score(X,Y)
# 0.8581798802065704
```

## 一个例子

酒的分类的数据集：

酒数据集中的178个样本被归入3个类别中，分别是class_0，class_1和class_2，其中class_0中包含59个样本，class_1中包含71个样本，class_2中包含48个样本。而从1）至13）分别是13个特征变量，包括酒精含量、苹果酸、镁含量、青花素含量、色彩饱和度等

```python
from sklearn.datasets import load_wine
load_wine().keys()
# 包括下面的信息
# dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names'])

# 包含178个样本, 每个样本有13个feature
# 178个标签
load_wine()['data'].shape # (178, 13)
load_wine()['target'].shape # (178, )
print(load_wine()['DESCR']) # 会输出每个标签的详细信息, 以及每个feature的简单统计量
```

### 分析

```python
# 构建训练集和测试集
# 使用split_train即可
# 默认情况下将其中75%的数据及所对应的标签划归到训练数据集，并将其余25%的数据和所对应的标签划归到测试数据集
# 我们一般用大写的X表示数据的特征，而用小写的y表示数据对应的标签。这是因为X是一个二维数组，也称为矩阵；而y是一个一维数组，或者说是一个向量

from sklearn.model_selection import train_test_splitin_test_split
from sklearn.datasets import load_wine
from sklearn.neighbors import KNeighborsClassifier

X_train, X_test, y_train, y_test = train_test_split(
                                    wine_dataset['data'],
                                    wine_dataset['target'], random_state=0)
# 查看一下每组数据集的情况
[getattr(x, "shape") for x in [X_train, X_test, y_train, y_test]]
# out
# [(133, 13), (45, 13), (133,), (45,)]
```

### 构建

```python
# 模型构建
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)
knn.score(X_test, y_test)
# out 
# 0.7333333333333333 r2还可以
# 预测一下

p = knn.predict(np.array(
    [[
        13.2, 2.77, 2.51, 18.5, 96.6, 1.04, 2.55, 0.57, 1.47, 6.2,1.05, 3.33, 820
    ]]
))
wine_dataset['target_names'][p]
# out
# array(['class_2'], dtype='<U7')
```

KNN需要对数据集认真地进行预处理、对规模超大的数据集拟合的时间较长、对高维数据集拟合欠佳，以及对于稀疏数据集束手无策等。所以在当前的各种常见的应用场景中，K最近邻算法的使用并不多见

KNN一般用的是
