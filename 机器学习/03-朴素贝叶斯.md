# 朴素贝叶斯

朴素贝叶斯（Naïve Bayes）算法是一种基于贝叶斯理论的有监督学习算法。之所以说“朴素”，是因为这个算法是基于样本特征之间互相独立的“朴素”假设。正因为如此，由于**不用考虑样本特征之间的关系，朴素贝叶斯分类器的效率是非常高的**

## 贝叶斯定理

$$
P(A|B) = \frac{P(B|A) \cdot P(A)}{P(B)}
$$

其中：

- \( P(A|B) \) 是在事件 B 已经发生的情况下事件 A 发生的**后验概率**（或条件概率）。
- \( P(B|A) \) 是在事件 A 已经发生的情况下事件 B 发生的**先验概率**（或条件概率）。
- \( P(A) \) 是事件 A 发生的先验概率。
- \( P(B) \) 是事件 B 发生的边缘概率。

这个例子感觉挺好..

```
已知：

天气预报说今日降水概率为50%——P（A）

晚高峰堵车的概率是80%——P（B）

如果下雨，晚高峰堵车的概率是95%——P（B|A）

小C向窗外望去，看到堵车了，则根据贝叶斯定理：

求得下雨的概率为0.5×0.95÷0.8=0.593 75。

小C果断地拿起雨伞冲了出去……
```

分类就是计算给定特征值的情况下，两个情况的概率是多少, 值大的那个就是对应的分类

## 例子

<img src="C:\Users\liuzihao\AppData\Roaming\Typora\typora-user-images\image-20240617174651860.png" alt="image-20240617174651860" style="zoom:80%;" />

例如这个气象信息：

7个sample，有四个特征，对应的lable是每天实际下雨的情况，0-没下雨；1-下雨了

*y* = [0, 1, 1, 0, 1, 0, 0]

```python
import numpy as np
X = np.array(
        [
            [0,1,0,1], # 0
            [1,1,1,0], # 1
            [0,1,1,0], # 2 
            [0,0,0,1], # 3
            [0,1,1,0], # 4
            [0,1,0,1], # 5
            [1,0,0,1]] # 6
    )
counts = {}
for label in np.unique(y):
    # counts储存的是当label=0/1(下雨或不下雨)时候每个特征出现的次数
    counts[label] = X[y==label].sum(axis = 0)
print(counts)
# out
# {0: array([1, 2, 0, 4]), 1: array([1, 3, 3, 0])}
#当y为0时，也就在没有下雨的4天当中，有1天刮了北风，有2天比较闷热，而没有出现多云的情况，但这4天天气预报全部播报有雨。同时，在y为1时，也就是在下雨的3天当中，有1天刮了北风，3天全都比较闷热，且3天全部出现了多云的现象，有意思的是，这3天的天气预报都没有播报有雨
```

## sklearn实现

```python
from sklearn.naive_bayes import BernoulliNB

clr = BernoulliNB()
clr.fit(X, y)

next_day = [[0,0,1,0]]
another_day = [[1,1,0,1]]
print(clr.predict(next_day))
print(clr.predict(another_day))
# out
# [1]
# [0]
# 查看预测的概率
print(clr.predict_proba(next_day))
print(clr.predict_proba(another_day))
# [[0.13848881 0.86151119]] 不下雨0.13 下雨0.86
# [[0.92340878 0.07659122]] 不下雨0.92 下雨0.07
```

## 几种朴素贝叶斯方法

贝努力朴素贝叶斯和多项式朴素贝叶斯只适合用来对非负离散数值特征进行分类，典型的例子就是对转化为向量后的文本数据进行分类，高斯朴素贝叶斯适用于接近正态分布的连续型变量分类

### 贝努力朴素贝叶斯

上述的天气的例子就是贝努力朴素贝叶斯，适合0-1分布或者二项分布的数据，使用包含数值的数据测试一下：

```python
from sklearn.datasets import make_blobs
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import BernoulliNB

X, y = make_blobs(n_samples=500, centers = 5, random_state=8)
print(X[0:3], y[0:3])
# out
#array([[-4.43344765, -9.14511574],
#        [-5.06998128, -9.75464122],
#        [ 6.54464509,  0.89987351]]),
# array([2, 2, 1]))

X_train,X_test, y_train, y_test = train_test_split(X, y, random_state=8)
nb = BernoulliNB()
nb.fit(X_train, y_train)
# 计算准确度
#res = nb.predict(X_test)
#np.sum( np.sum(res==y_test)/len(res))
nb.score(X_test, y_test)
# 0.54
```

上面的例子可以看出仅仅有0.54的准确率

贝努力贝叶斯的分类过程：

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240618160146407.png" alt="image-20240618160146407" style="zoom:67%;" />

我们使用了贝努利朴素贝叶斯的默认参数binarize=0.0，所以模型对于数据的判断是，如果特征1大于或等于0，且特征2大于或等于0，则将数据归为一类；如果特征1小于0，且特征2也小于0，则归为另一类而其余的数据全部归为第三类，所以分类错误率太高

```
binarize : float or None, default=0.0
    Threshold for binarizing (mapping to booleans) of sample features.
    If None, input is presumed to already consist of binary vectors
对于连续行数据来说不适用... 如果改成2, 特征1大于或等于2，且特征2大于或等于2，则将数据归为一类；如果特征1小于2，且特征2也小于2，则归为另一类而其余的数据全部归为第三类
```

### 高斯朴素贝叶斯

```python
from sklearn.naive_bayes import GaussianNB
gb = GaussianNB()
gb.fit(X_train, y_train)
gb.score(X_test, y_test)
# out
# 0.968
```

可以看到适用于正态分布数据的高斯贝叶斯是的准确率大大提升

看一下分类边界：

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240618162312263.png" alt="image-20240618162312263" style="zoom:67%;" />

**高斯朴素贝叶斯也确实是能够胜任大部分的分类任务，这是因为在自然科学和社会科学领域，有大量的现象都是呈现出正态分布的状态**

### 多项式朴素贝叶斯

多项式分布：

我们知道硬币只有两个面，正面和反面，而骰子有6个面，因此每掷一次骰子，结果都可能是从1～6这6个数字，如果我们掷*n*次骰子，而每个面朝上的次数的分布情况，就是一个多项式分布

对于多项式朴素贝叶斯来说，输入的数据必须每个feature大于等于0

```python
from sklearn.naive_bayes import MultinomialNB
from sklearn.preprocessing import MinMaxScaler # 数据中存在小于0的值，因此进行max-min转换, 缩放到0-1的范围
#max-min转换:最简单的转换
scaler = MinMaxScaler()
scaler.fit(X)
X_test_scale = scaler.transform(X_test)
X_train_scale = scaler.transform(X_train)

mb = MultinomialNB()
mb.fit(X_train_scale, y_train)
mb.score(X_test_scale, y_test)
# out
# 0.312
```

可以看出多项式朴素贝叶斯并不适用上述数据

**多项式朴素贝叶斯只适合用来对非负离散数值特征进行分类，典型的例子就是对转化为向量后的文本数据进行分类**

## 肿瘤数据实践

569个样本, 30个feature

```python
## 肿瘤数据
from sklearn.datasets import load_breast_cancer
from sklearn.naive_bayes import GaussianNB
from sklearn.model_selection import train_test_split
cancer = load_breast_cancer()

# data记录了每个肿瘤的特征值(都是连续型变量), target是分型
cancer.keys()
cancer.feature_names[0:3]
# out
# array(['mean radius', 'mean texture', 'mean perimeter'], dtype='<U23')
X_train, X_test, y_train, y_test = train_test_split(cancer['data'], cancer['target'], random_state=8)
gb = GaussianNB()
gb.fit(X_train, y_train)
print(gb.score(X_test, y_test))
# out
# 0.9370629370629371
# 随便预测几个
print(gb.predict_proba([cancer['data'][100]]))
print(gb.predict([cancer['data'][100]]))
# out 
# 1
print(cancer['target'][100])
# out  发现了一个预测错的2333
# 0
```

可以看出高斯朴素贝叶斯可以较好的对肿瘤数据进行分类, 模型的准确率为0.93

## 学习曲线

通过学习曲线可以去评估在每个样本量下模型的准确率是怎样的：

```python
from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt
from sklearn.model_selection import ShuffleSplit # n折交叉验证

# learning_curve
# 会输出三个值,分别为每组拆分的样本数量, 每组训练集的准确率, 每组测试集的准确率

# cv: 默认做5折交叉验证：
#将原始数据集分成五个相等大小的子集（或折叠），其中四个子集用于训练模型，而剩下的一个子集用于测试模型。这个过程重复五次，每次选择不同的一个子集作为测试集，其余的作为训练集。最后，将五次的性能评估结果取平均值以得到最终评估结果, 本次是做的是拆成100个子集

train_size, train_scores, test_scores = learning_curve(
    GaussianNB(), 
    cancer['data'], 
    cancer['target'], 
    n_jobs=1, 
cv = ShuffleSplit(n_splits = 100, test_size = 0.2, random_state=0))

# train_size
# array([ 45, 147, 250, 352, 455])
len(train_scores)
# 100 输出100个准确率，我们需要对其求均值, 测试集同理
```

简易绘制学习曲线：

```python
plt.ylim(0.9, 1.01)
plt.plot(train_size, train_scores.mean(axis = 1))
plt.plot(train_size, test_scores.mean(axis = 1))
# 蓝色为训练集的学习曲线
# 橙色为测试集
# x-样本大小
# y-模型准确率
```

在训练数据集中，随着样本量的增加，模型的得分是逐渐降低的。这是因为随着样本数量增加，模型要拟合的数据越来越多，难度也越来越大。而模型的交叉验证得分的变化相对没有那么明显，从10个样本左右一直到接近500个样本为止，分数一直在0.94左右浮动。这说明高斯朴素贝叶斯在预测方面，对于样本数量的要求并没有那么苛刻。所以如果你的样本数量比较少的话，应该可以考虑使用朴素贝叶斯算法来进行建模

![image-20240618173113689](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240618173113689.png)

## 总结

相比起线性模型算法来说，朴素贝叶斯算法的效率要高一些，这是因为朴素贝叶斯算法会把数据集中的各个特征看作完全独立的，而不考虑特征之间的关联关系。但同时模型泛化的能力会稍微弱一点，不过一般情况下并不太影响实际的使用。尤其是在现在这个大数据时代，很多数据集的样本特征可能成千上万，这种情况下，模型的效率要比模型泛化性能多零点几个百分点的得分重要得多。在这种超高维度的数据集中，训练一个线性模型的时间可能会非常长，因此在这种情况下，朴素贝叶斯算法往往是一个更好的选择。

## 关于ShuffleSplit

```
Init signature:
ShuffleSplit(
    n_splits=10,
    *,
    test_size=None,
    train_size=None,
    random_state=None,
)
Docstring:     
Random permutation cross-validator # 它通过随机的方式将数据集划分为训练集和测试集

Yields indices to split data into training and test sets.

Note: contrary to other cross-validation strategies, random splits
do not guarantee that all folds will be different, although this is
still very likely for sizeable datasets.

Read more in the :ref:`User Guide <ShuffleSplit>`.

Parameters
----------
n_splits : int, default=10
    Number of re-shuffling & splitting iterations.

test_size : float or int, default=None
    If float, should be between 0.0 and 1.0 and represent the proportion
    of the dataset to include in the test split. If int, represents the
    absolute number of test samples. If None, the value is set to the
    complement of the train size. If ``train_size`` is also None, it will
    be set to 0.1.

train_size : float or int, default=None
    If float, should be between 0.0 and 1.0 and represent the
    proportion of the dataset to include in the train split. If
    int, represents the absolute number of train samples. If None,
    the value is automatically set to the complement of the test size.

random_state : int, RandomState instance or None, default=None
    Controls the randomness of the training and testing indices produced.
    Pass an int for reproducible output across multiple function calls.
    See :term:`Glossary <random_state>`.
```
