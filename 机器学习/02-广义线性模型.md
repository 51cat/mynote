# 广义线性模型

线性模型并不是特指某一个模型，而是一类模型。在机器学习领域，常用的线性模型包括线性回归、岭回归、套索回归、逻辑回归和线性SVC等,模型给出的预测y可以看作输入特征的加权和，而*w*参数就代表了每个特征的权重，当然，*w*也可以是负数

![image-20240614133115906](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240614133115906.png)

线性模型的公式一般如上：

其中*x*[0]，*x*[1]，…，*x*[p]为数据集中特征变量的数量（这个公式表示数据集中的数据点一共有*p*个特征）；*w*和*b*为模型的参数；*ŷ*为模型对于数据结果的预测值。

如果只有一个特征那么就可以简化为

![image-20240614133212724](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240614133212724.png)

## 线性回归

线性回归，也称为普通最小二乘法（OLS），是在回归分析中最简单也是最经典的线性模型

线性回归的原理是，找到当训练数据集中*y*的预测值和其真实值的平方差最小的时候，所对应的*w*值和*b*值。线性回归没有可供用户调节的参数，这是它的优势，但也代表我们无法控制模型的复杂性。

首先使用fake实现一下线性回归的预测

```python
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.datasets import make_regression

# 构建一个feature为2, 样本数为100的数据集
X, y = make_regression(n_features=2, n_samples = 100, n_informative=2, random_state=8)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=8)
lr = LinearRegression()
lr.fit(X_train, y_train)

print(lr.coef_)
print(lr.intercept_)
# 性能
print("训练数据集得分：{:.2f}".format(lr.score(X_train, y_train)))
print("测试数据集得分：{:.2f}".format(lr.score(X_test, y_test)))
# output
# [82.06262736  7.73658023] 两个feature的系数值
# 6.661338147750939e-16  截距值
# 因为没有噪音, 所以数据拟合的特别好
# 训练数据集得分：1.00
# 测试数据集得分：1.00
```

在本例中线性回归模型的方程可以表示为，截距可以理解为误差

![image-20240614140848869](https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240614140848869.png)

// **coef_和intercept_这两个属性非常奇怪，它们都是以下划线_结尾。这是sciki-learn的一个特点，它总是用下划线作为来自训练数据集的属性的结尾，以便将它们与由用户设置的参数区分开**

使用一个真实数据集预测共10个feature，下面的代码可以看出模型的得分并不高

```python
from sklearn.datasets import load_diabetes
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.datasets import make_regression

X, y = load_diabetes().data, load_diabetes().target 
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=8)
lr = LinearRegression()
lr.fit(X_train, y_train)

print(lr.coef_)
print(lr.intercept_)

# output
#[   11.51226671  -282.51443231   534.2084846    401.73037118
# -1043.90460259   634.92891045   186.43568421   204.94157943
#   762.46336088    91.95399832]
# 152.5625670974632

print("训练数据集得分：{:.2f}".format(lr.score(X_train, y_train)))
print("测试数据集得分：{:.2f}".format(lr.score(X_test, y_test)))

# 训练数据集得分：0.53
# 测试数据集得分：0.46
```

由于线性回归自身的特点，**非常容易出现过拟合的现象**。在训练集的得分和测试集的得分之间存在的巨大差异是出现过拟合问题的一个明确信号，因此，我们应该找到一个模型，使我们能够控制模型的复杂度。标准线性回归最常用的替代模型之一是岭回归

## 岭回归（L2正则化）

岭回归实际上是一种能够避免过拟合的线性模型。在岭回归中，**模型会保留所有的特征变量，但是会减小特征变量的系数值**，让特征变量对预测结果的影响变小，在岭回归中是通过改变其**alpha参数**（惩罚系数）来控制减小特征变量系数的程度。而这种通过保留全部特征变量，只是降低特征变量的系数值来避免过拟合的方法，我们称之为**L2正则化**

使用同样的数据基于岭回归进行建模

```python
from sklearn.linear_model import Ridge
ridge = Ridge()
ridge.fit(X_train, y_train)
print(rlr.coef_)
print(rlr.intercept_)
# out 可以看出确实是缩小了好几个参数, 误差项的变化不大
#[  36.82662813  -75.80807883  282.42734567  207.39238457   -1.46557355
#  -27.81684598 -134.37462416   98.97793127  222.67324438  117.97352343]
#152.55358561069758

# 性能
print(rlr.score(X_test, y_test))
print(rlr.score(X_train, y_train))
# out
# 0.4325221097526062
# 0.4326365837100664
```

可以看出岭回归在训练集和测试集上的表现差不多，与线性回归相比具有更强的泛化能力

岭回归是在模型的简单性（使系数趋近于零）和它在训练集上的性能之间取得平衡的一种模型。用户可以使用alpha参数控制模型更加简单性还是在训练集上的性能更高。在上一个示例中，我们使用默认参数alpha = 1，**alpha的取值并没有一定之规。alpha的最佳设置取决于我们使用的特定数据集。增加alpha值会降低特征变量的系数，使其趋于零，从而降低在训练集的性能，但更有助于泛化**，alpha的值越小，岭回归的结果越接近线性回归

```python
# 设置一个比较极端的值
rlr = Ridge(alpha=100)
rlr.fit(X_train, y_train)
print(rlr.coef_)
print(rlr.intercept_)
print(rlr.score(X_test, y_test))
print(rlr.score(X_train, y_train))
# out
# [ 1.9727495   0.18165568  7.07146474  5.57033905  2.23127867  1.71615027
# -4.34680543  4.64889079  6.54400782  4.68662919]
# 152.8375982689721
# 0.020705982352898444
# 0.020254491804037267
```

可视化一下不同的alpha值对系数的影响：例如

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240614153305687.png" alt="image-20240614153305687" style="zoom:80%;" />



那就是取一个**固定的alpha值**，然后改变训练数据集的数据量。比如我们在糖尿病数据集中采样，然后用这些采样的子集对线性回归模型和alpha值等于1的岭回归模型进行评估，并用jupyter notebook进行绘图，得到一个随数据集大小而不断改变的模型评分折线图，其中的折线我们也称之为学习曲线（learning curves）

```python
from sklearn.model_selection import learning_curve,KFold
import numpy as np

def plot_learning_curve(est, X, y):
    training_set_size, train_score, test_score = learning_curve(
        est, X, y, train_sizes=np.linspace(0.1, 1, 20), 
        cv=KFold(20, shuffle=True, random_state=1)
    
    )
    
    line = plt.plot(training_set_size, train_score.mean(axis=1), "--")
    plt.plot(training_set_size, train_score.mean(axis=1), "-")
plot_learning_curve(Ridge(alpha = 1), X, y)
plot_learning_curve(LinearRegression(), X, y)
```

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240614155835167.png" alt="image-20240614155835167" style="zoom:80%;" />

可以看出随着样本规模不断加大，线性回归和岭回归的差异渐渐减小，数据规模足够大的时候，二者的差异就很小了，但是数据规模很大的时候线性模型仍然会出现过拟合的现象

## lasso回归（L1正则化）

套索回归也会将系数限制在非常接近0的范围内，但它进行限制的方式稍微有一点不同，我们称之为**L1正则化**。与L2正则化不同的是，L1正则化会导致在使用套索回归的时候，有一部分特征的系数会正好等于0。也就是说，**有一些特征会彻底被模型忽略掉，这也可以看成是模型对于特征进行自动选择的一种方式。把一部分系数变成0有助于让模型更容易理解，而且可以突出体现模型中最重要的那些特征**

继续用上面的数据集

```python
from sklearn.linear_model import Lasso
lasso = Lasso()
lasso.fit(X_train, y_train)

print(lasso.coef_)
print(lasso.intercept_)

# out 可以看出狠多参数都是0，使用了L1正则化, 只使用了3个feature
#[  0.          -0.         384.73866871  72.69163139   0.
#   0.          -0.           0.         247.88294779   0.        ]
#152.6882652822777

print(lasso.score(X_train, y_train))
print(lasso.score(X_test, y_test))
# out 
# 0.3624222204154225
# 0.36561940472905163
```

为了降低欠拟合的程度，我们可以试着**降低alpha的值**。与此同时，我们还需要**增加最大迭代次数（max_iter）的默认设置**

```python
lasso = Lasso(alpha=0.25, max_iter=200000)
lasso.fit(X_train, y_train)
print(lasso.score(X_train, y_train))
print(lasso.score(X_test, y_test))
# out 均有所提升, 但有一点过拟合的现象 
# 0.4925506342101602
# 0.4772598041593238
```

降低alpha值可以拟合出更复杂的模型，从而在训练数据集和测试数据集都能获得良好的表现。相对岭回归，lasso回归的表现还要稍好一点，而且它只用了一部分，这一点也会使模型更容易被人理解，不过过低的alpha值会出现过拟合

## 不同alpha的岭回归和lasso回归系数的对比

<img src="https://51catgithubio.oss-cn-beijing.aliyuncs.com/image-20240614161749700.png" alt="image-20240614161749700" style="zoom:67%;" />

```
从图中我们不难看出，当alpha值等于1的时候，不仅大部分系数为0，而且仅存的几个非零系数数值也非常小。把alpha值降低到0.01时，如图中正三角形所示，大部分系数也是0，但是等于0的系数已经比alpha等于1的时候少了很多。而当我们把alpha值降低到0.000 1的时候，整个模型变得几乎没有被正则化，大部分系数都是非零的，并且数值变得相当大。作为对比，我们能看到圆点代表的是岭回归中的系数值。alpha值等于0.1的岭回归模型在预测能力方面基本与alpha值等于0.1的套索回归模型一致，但你会发现，使用岭回归模型的时候，几乎所有的系数都是不等于0的
```

实际需求中，岭回归往往更常用，但是如果确定数据中只有部分的feature起作用，那么还是可以用lasso回归的

**scikit-learn还提供了一种模型，称为弹性网模型（Elastic Net）。弹性网模型综合了套索回归和岭回归的惩罚因子。在实践中这两个模型的组合是效果最好的，然而代价是用户需要调整两个参数，一个是L1正则化参数，另一个是L2正则化参数**

```python
# 例如
a = ElasticNet(alpha=0.02, max_iter=20000)
a.fit(X_train, y_train)
a.score(X_train, y_train)
a.score(X_test, y_test)
```

如果你的数据集有很多特征，而这些特征中并不是每一个都对结果有重要的影响，那么就应该使用L1正则化的模型，如套索回归；但如果数据集中的特征本来就不多，而且每一个都有重要作用的话，那么就应该使用L2正则化的模型，如岭回归
